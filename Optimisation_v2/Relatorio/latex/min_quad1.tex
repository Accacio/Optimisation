O \textit{Método dos Mínimos Quadrados} ) é uma técnica de otimização matemática que procura encontrar o melhor ajuste para um conjunto de dados tentando minimizar a soma dos quadrados das diferenças entre o valor estimado e os dados observados (tais diferenças são chamadas resíduos) e consiste em um estimador que minimiza a soma dos quadrados dos resíduos da regressão, de forma a maximizar o grau de ajuste do modelo aos dados observados.

Um requisito para o método dos mínimos quadrados é que o fator imprevisível (erro) seja distribuído aleatoriamente, essa distribuição seja normal e independente. Outro requisito é que o modelo é linear nos parâmetros, ou seja, as variáveis apresentam uma relação linear entre si. Caso contrário, deveria ser usado um modelo de \textit{regressão não-linear}.


Caso alguma dessas premissas não seja verdadeira, o método pode gerar resultados sub-ótimos ou com viés.

\mysubsection{Regressão Simples}
Queremos estimar valores de determinada variável $y$. Para isso, consideramos os valores de outra variável $x$ que acreditamos ter poder de explicação sobre $y$ conforme a fórmula:

\begin{equation}
	y = \alpha + \beta x + \epsilon,
\end{equation}

\begin{itemize}
	\item $\alfa $: Parâmetro do modelo chamado de constante (porque não depende de $x$)
	\item $\beta$ :  Parâmetro do modelo chamado de coeficiente da variável $x$
	\item $\epsilon$ : Erro - representa a variação de $y$, que não é explicada pelo modelo.
\end{itemize}

Também temos uma base de dados com n valores observados de $y$ e de $x$. Perceba que, usando a base de dados,$x$ e $y$ são vetores, ou seja, representam uma lista de valores, um para cada observação da base de dados. O método dos mínimos quadrados ajuda a encontrar as estimativas de $\alpha$ e $\beta$. Como o nome diz, serão somente estimativas desses parâmetros, porque o valor real dos parâmetros são desconhecidos. Portanto, ao fazer a estimativa, mudamos a notação de algumas variáveis:
\begin{itemize}
	\item $\alfa \rightarrow a$
	\item $\beta \rightarrow b$
	\item $\epsilon \rightarrow e$
\end{itemize}

Desse modo, ao estimar o modelo usando a base de dados, estamos estimando, na verdade:
\begin{equation}
	y_i = a + bx_i + e_i
\end{equation}

O método dos mínimos quadrados minimiza a soma dos quadrado dos resíduos, ou seja, minimiza $\sum_{i=1}^{n}e^2_i$

A ideia por trás dessa técnica é que, minimizando a soma do quadrado dos resíduos, encontraremos $a$ e $b$ que trarão a menor diferença entre a previsão de $y$ e o $y$ realmente observado.
Com algumas manipulações matemáticas podemos facilmente chegar à fórmula:
\begin{equation}
	b=\frac{\sum_{i=1}^{n}x_i(y_i-\bar{y})}{\sum_{i=1}^{n}x_i(x_i-\bar{x})}
\end{equation}

\mysubsection{Regressão múltipla}
A regressão múltipla apresenta um funcionamento parecido com o da regressão simples, porém, leva em consideração diversas variáveis explicativas $x$ influenciando $y$ ao mesmo tempo:
\begin{equation}
	y = \beta_0 + x_1\beta_1 + x_2\beta_2 + x_3\beta_3 + ... + x_k\beta_k + \epsilon
\end{equation}

Ao usar a base de dados com $k$ variáveis explicativas e $n$ observações, o modelo pode ser escrito na forma matricial:
\begin{equation}
\begin{Bmatrix}
y_1 \\
y_2 \\
y_3 \\
y_4 \\
\dots \\
y_n 
\end{Bmatrix}
=
\begin{Bmatrix}
1 & x_{11} & x_{21} & \dots & x_{k1} \\
1 & x_{12} & x_{22} & \dots & x_{k1} \\
1 & x_{13} & x_{23} & \dots & x_{k1} \\
1 & x_{14} & x_{24} & \dots & x_{k1} \\
\dots & \dots & \dots & \dots & \dots \\
1 & x_{1n} & x_{2n} & \dots & x_{k1} \\
\end{Bmatrix}
\times
\begin{Bmatrix}
b_0 \\
b_1 \\
b_2 \\
\dots \\
b_k
\end{Bmatrix}
+
\begin{Bmatrix}
e_1 \\
e_2 \\
e_3 \\
e_4 \\
\dots \\
e_n
\end{Bmatrix}
\end{equation}

Onde $x_{ji}$ representa o valor da j-ésima variável da i-ésima observação. A fórmula também pode ser escrita na forma resumida:
\begin{equation}
	y = Xb + e
\end{equation}
A solução de mínimos quadrados continua sendo alcançada através da minimização da soma do quadrado dos resíduos.
A minimização se dá ao derivar $S(b)$ em relação a $b$ e igualar a zero. O primeiro termo não depende de $b$, os segundo e terceiro termos são iguais e o terceiro termo é uma forma quadrática dos elementos de $b$.

\begin{equation}
	b=(X'X)^{-1}X'y
\end{equation}


\mysubsection{Premissas}
Ao usar o método dos mínimos quadrados, assumimos algumas premissas a respeito das variáveis:
\begin{itemize}
	\item \textbf{Os regressores são fixos:} As variáveis da matriz X não são estocásticas.
	\item \textbf{Erro é aleatório com média 0:} O erro $\epsilon$ é aleatório e sua esperança $E(\epsilon)=0$.
	\item\textbf{Homoscedasticidade:} A variância do erro é constante.
	\item\textbf{Sem correlação:} Não existe correlação entre os erros das observações, ou seja, $E(\epsilon_i \epsilon_j)=0$ para qualquer $i\neq j$.
	\item \textbf{Parâmetros são constantes:} $\alfa$ e $\beta$ são valores fixos desconhecidos.
	\item \textbf{Modelo é linear:} Os dados da variável dependente $y$ foram gerados pelo processo linear $y=X\beta + \epsilon$.
	\item \textbf{Erro tem distribuição normal:} O erro é distribuído conforme a curva de distribuição normal.	
\end{itemize}


\newpage
